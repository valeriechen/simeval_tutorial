{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_types = ['lime']\n",
    "train_sizes = [4, 16, 64, 100, 1000]\n",
    "random_seeds = [0, 1, 2]\n",
    "val_size = 250\n",
    "weight_decay = 1e-4\n",
    "lr = 1e-4\n",
    "epochs = 1000\n",
    "batchsize = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to define agent architecture. We show one possible architecture (the DeepSet model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "\n",
    "    def __init__(self, set_size, in_features, set_features=100): #50\n",
    "        super(DeepSet, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = set_features\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_features, 200), #100\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(200, 100), #100\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(100, set_features)\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(set_features, 30),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(30, 30),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(30, 10),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.weighted_sum = nn.Conv1d(in_channels=set_size, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.add_module('0', self.feature_extractor)\n",
    "        self.add_module('1', self.regressor)\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for module in self.children():\n",
    "            reset_op = getattr(module, \"reset_parameters\", None)\n",
    "            if callable(reset_op):\n",
    "                reset_op()\n",
    "            \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.sum(dim=1) \n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'Feature Exctractor=' + str(self.feature_extractor) \\\n",
    "            + '\\n Set Feature' + str(self.regressor) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDeepset(Dataset):\n",
    "    def __init__(self, n, X, y, input_dim: int):\n",
    "        self.n = n\n",
    "        #load files\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.X[index,:]\n",
    "        item = item.reshape((1, self.input_dim))\n",
    "        return torch.from_numpy(item).type(torch.FloatTensor), torch.from_numpy(np.array([self.y[index]])).type(torch.FloatTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_by_ind(ind):\n",
    "    X = d['x'][ind]\n",
    "    y = d['y'][ind].flatten()\n",
    "    u = d['u'][ind].flatten()\n",
    "    lime = d['lime'][ind]\n",
    "    \n",
    "    return X, y, u, lime\n",
    "\n",
    "def load_train_and_test_data(N_train, N_test, ind):\n",
    "      \n",
    "    train_ind_subset = ind[:N_train]\n",
    "    test_ind_subset = ind[-N_test:]\n",
    "        \n",
    "    X_train, y_train, u_train, lime_train = data_by_ind(train_ind_subset)\n",
    "    \n",
    "    X_test, y_test, u_test, lime_test = data_by_ind(test_ind_subset)\n",
    "        \n",
    "    return X_train, y_train, u_train, lime_train, X_test, y_test, u_test, lime_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in random_seeds:\n",
    "    print(f'seed {seed}')\n",
    "    \n",
    "    # default rs 0: select the first N/2 points from each dataframe\n",
    "    ind = np.arange(original_len(d))\n",
    "    \n",
    "    if seed != 0:\n",
    "        # randomly shuffle the order in which we select from the dataframes\n",
    "        np.random.shuffle(ind)\n",
    "\n",
    "    for train_size in train_sizes:\n",
    "        print(f'size {train_size}')\n",
    "        X_train, y_train, u_train, lime_train, shap_train, gam_train, X_test, y_test, u_test, lime_test, shap_test, gam_test = load_train_and_test_data(N_train = train_size, N_test = 250, ind = ind)\n",
    "        \n",
    "        for exp_str in exp_types:\n",
    "            # only use a train set of the given size\n",
    "            exp_train = None\n",
    "            exp_test = None\n",
    "            if exp_str == 'lime':\n",
    "                exp_train = lime_train\n",
    "                exp_test = lime_test\n",
    "\n",
    "            train_loader, train_dim, X_train_tensor, u_train_tensor = get_loader(X_train, u_train, \n",
    "                                     exp = exp_train, \n",
    "                                     batchsize = batchsize)\n",
    "\n",
    "            test_loader, test_dim, X_test_tensor, u_test_tensor = get_loader(X_test, u_test, \n",
    "                                           exp = exp_test,\n",
    "                                           batchsize = batchsize)\n",
    "            \n",
    "            model = DeepSet(set_size = 1, \n",
    "                in_features = train_dim)\n",
    "\n",
    "            model.to(device=device)\n",
    "            \n",
    "            model.train_accs = []\n",
    "            model.test_accs = []\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            \n",
    "            # Defining Loss Function\n",
    "            criterion = nn.BCELoss()\n",
    "\n",
    "            # Define Training Loop\n",
    "            def train(epoch, loss_list):\n",
    "                model.train()\n",
    "                for batch_idx, (image, mask) in enumerate(train_loader):\n",
    "                    image = image.to(device=device)\n",
    "                    mask = mask.to(device=device)\n",
    "                    mask = mask.squeeze(1)\n",
    "                    image, mask = Variable(image), Variable(mask)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    output = model(image)\n",
    "\n",
    "                    loss = criterion(output, mask.unsqueeze(1)) \n",
    "                    loss_list.append(loss.item())\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            def evaluate_accuracy(X_tensor, u_true_tensor, is_train):\n",
    "                u_pred_soft = model(X_tensor)\n",
    "                u_pred_hard = np.round(u_pred_soft.detach()).flatten()\n",
    "                acc = accuracy_score(u_true_tensor.detach().flatten(), u_pred_hard)\n",
    "                \n",
    "                if is_train:\n",
    "                    model.train_accs.append(acc)\n",
    "                else:\n",
    "                    model.test_accs.append(acc)\n",
    "\n",
    "\n",
    "            loss_list = []\n",
    "            for i in range(epochs):\n",
    "                train(i, loss_list)\n",
    "                evaluate_accuracy(X_train_tensor, u_train_tensor, is_train = True)\n",
    "                evaluate_accuracy(X_test_tensor, u_test_tensor, is_train = False)\n",
    "\n",
    "\n",
    "            # store trained model\n",
    "            torch.save(model.state_dict(),\n",
    "                       f'models/{version_substr}_rs{seed}_wd={weight_decay}_model_trainsize_{train_size}_exptype_{exp_str}.pt')\n",
    "\n",
    "            # dump validation accuracies and train accuracies\n",
    "            pickle.dump(model.train_accs, open(f'train_accs.pt', \"wb\" ) )\n",
    "            pickle.dump(model.test_accs, open(f'test_accs.pt', \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
